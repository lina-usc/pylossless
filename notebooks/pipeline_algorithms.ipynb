{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyLossless pipeline: A step-by-step demonstration\n",
    "\n",
    "This Notebook details each pipeline step. First, the mathematical notation for the operation will be described. Second, the python code for the operation will be shown. We will go through the first step in details. Similar procedures are used in all following steps so for the sake of brevity we will increasingly explain less of the formulas throughout this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning and installing pylossless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE ONLY NEEDS TO BE RUN ONCE.\n",
    "# IF PYLOSSLESS IS ALREADY AVAILABLE, IT DOES NOT NEED TO BE RERUN.\n",
    "try:\n",
    "  import pylossless\n",
    "except ModuleNotFoundError:\n",
    "  !git clone https://github.com/lina-usc/pylossless.git\n",
    "  %pip install --quiet --editable ./pylossless\n",
    "\n",
    "  # RESTARTING THE KERNEL AFTER THE PIP INSTALL\n",
    "  # THIS IS NECESSARY FOR THE NEWLY INSTALLED PACKAGE\n",
    "  # TO BE CORRECTLY IMPORTED.\n",
    "  exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "--------------------------------\n",
    "# Getting a LosslessPipeline object\n",
    "\n",
    " Create a pipeline object and manually set a raw object to it. We proceed this way to be able to demonstrate the operation for each step, one at a time. Normally, one would simply load and process a raw object with `pipeline.run_with_raw(raw)`. This demonstration relies on test datasets from MNE. It may take a while to download the MNE sample data (about 2-3 minutes on Google Colab). **Please be patient**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mne\n",
    "from mne.datasets import sample\n",
    "\n",
    "import pylossless as ll\n",
    "\n",
    "# LOAD DATA FROM MNE FOR THE DEMONSTRATION\n",
    "data_path = sample.data_path()\n",
    "meg_path = data_path / 'MEG' / 'sample'\n",
    "raw_fname = meg_path / 'sample_audvis_raw.fif'\n",
    "raw = mne.io.read_raw_fif(raw_fname, preload=True)\n",
    "\n",
    "# LOAD DEFAULT CONFIG\n",
    "config = ll.config.Config()\n",
    "config.load_default()\n",
    "config.save(\"test_config.yaml\")\n",
    "\n",
    "# CREATE A PIPELINE AN MANUALLY SET A RAW OBJECT\n",
    "pipeline = ll.LosslessPipeline('test_config.yaml')\n",
    "pipeline.raw = raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble: Notation\n",
    "\n",
    "We define:\n",
    "- s, e, and t, as sensor, epochs, and time dimensions, respectively.\n",
    "\n",
    "- a 3D matrix $X \\in \\mathbb{R}^{S_\\mathcal{G} \\times E_\\mathcal{G} \\times T}$, where $S_\\mathcal{G}$  and $E_\\mathcal{G}$ are the set of good sensors and epochs respectively, and $T$ is the number of time points (i.e. samples)\n",
    "\n",
    "- We use superscript to denote operations across a dimension, and we use subscript to denote indexing a dimension\n",
    "\n",
    "\n",
    "- a single sensor $i$ as $ X\\big|_{s=i} \\in \\mathbb{R}^{E_\\mathcal{G} \\times T}$, with $i \\in S_\\mathcal{G}$\n",
    "\n",
    "- a single epoch $j$ as  $X\\big|_{e=j} \\in \\mathbb{R}^{S_\\mathcal{G} \\times T}$, with $j \\in E_\\mathcal{G} $\n",
    "\n",
    "- sensor specific thresholds for rejecting epochs as $\\tau^e_i \\in \\mathbb{R}^{S_\\mathcal{G}}$\n",
    "\n",
    "- epoch specific thresholds for rejecting sensors as $\\tau^s_j \\in \\mathbb{R}^{E_\\mathcal{G}}$\n",
    "\n",
    "\n",
    "- *quantiles* as $Q\\#^{dim}$: i.e. $Q75^s$ is the 75th *quantile* along the sensor dimension. The function $Q75^s(X)$ computes the 75th quantile along the $s$ dimension of matrix $X$, resulting in a matrix noted $X^{Q75^s} \\in \\mathbb{R}^{E \\times T}$\n",
    "\n",
    "Throughout the text, we use capital letters for matrices and lower-case letters for scalars. For example, the data point for sensor $i$, epoch $j$ and time $k$ as $X\\big|_{s=i; e=j; t=k} = x_{ijk}\\in \\mathbb{R}$, and $X=\\{x_{ijk}\\}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we epoch the data to be used for step 1\n",
    "\n",
    "Let our 3D matrix below be defined as $X \\in \\mathbb{R}^{S \\times E \\times T}$ where $X$ is a matrix of real numbers and of dimension $S$ sensors $\\times$ $E$ epochs $\\times$ $T$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = pipeline.get_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us convert our epochs array into a Named Array\n",
    "from pylossless.pipeline import epochs_to_xr\n",
    "epochs_xr = epochs_to_xr(epochs, kind='ch')\n",
    "epochs_xr # 277 epochs, 50 sensors, 602 samples per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Flag Noisy Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/Flag_noisy_sensors.png\" alt=\"Flag noisy channels\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## step 1a) Take standard deviation across temporal dimension\n",
    "\n",
    "#### We take the standard deviation of $X \\in \\mathbb{R}^{S \\times E \\times T}$ across the samples dimension time, resulting in a 2D matrix $X^{\\sigma_{t}} \\in \\mathbb{R}^{S \\times E}$\n",
    "\n",
    "\n",
    "![std across samples](https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/samples_std.png)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take standard deviation across time dimension\n",
    "trim_ch_sd = epochs_xr.std('time')\n",
    "# display a small subset\n",
    "trim_ch_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the 50th and 75th quantile across dimension sensor of $X^{\\sigma_{t}}$\n",
    "\n",
    "\n",
    "This operation results in two 1D vectors of size $E$:\n",
    "\n",
    "- $ X^{{\\sigma}_t{Q50^s}} = Q50^s(X^{\\sigma_{t}}) \\in \\mathbb{R}^{E} $\n",
    "- $ X^{{\\sigma}_t{Q75^s}} = Q75^s(X^{\\sigma_{t}}) \\in \\mathbb{R}^{E} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step a\n",
    "q50, q75 = trim_ch_sd.quantile([.5, .75], dim='ch')\n",
    "q50 # a 1D array of median stDev values across channels for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) Let the *Upper Quantile Range* be defined by $Q75 - Q50$\n",
    "$ UQR^s = X^{{\\sigma}_T{Q75}^s} - X^{{\\sigma}_T{Q50}^s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step B. Define the threshold\n",
    "uqr = q75 - q50\n",
    "uqr  # Q70 - Q50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify outlier Indices $(i, j)$\n",
    "\n",
    "We multiply a constant $k$ by the $UQR$ to define a measure for the spread of the right tail of the distribution of $X^{\\sigma_{t}}$ values and add it to the median of $X^{\\sigma_{t}}$ to obtain epoch-specific standard deviation threshold for outliers:\n",
    "\n",
    "$$\n",
    "    \\tau^s_j = X^{{\\sigma}_T{Q50}^S} + UQR^s\\times k\n",
    "$$\n",
    "\n",
    "That is, $\\tau^s_j$ is the epoch-specific threshold for the epoch $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1b part 2\n",
    "k = 3\n",
    "u_out =  q50 + q75 * k\n",
    "u_out  # epoch specific thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compare our 2D standard deviation matrix to the threshold vector of $\\tau^e_j$:\n",
    "\n",
    "$$\n",
    "X^{\\sigma_{t}} \\big|_{e=j}  > \\tau^s_j\n",
    "$$\n",
    "\n",
    "resulting in the indicator matrix $C \\in \\{0, 1\\}^{S \\times E}=\\{c_{ij}\\}$ with\n",
    "\n",
    "$$\n",
    "c_{ij} =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } x^{\\sigma_{t}}_{ij} < \\tau^s_j \\\\\n",
    "1 & \\text{if } x^{\\sigma_{t}}_{ij} \\geq \\tau^s_j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Each element of this matrix indicates whether sensor $i$ is an outlier at an epoch $j$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step D\n",
    "outlier_mask = trim_ch_sd > u_out\n",
    "outlier_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c) Identify noisy sensors part 1\n",
    "To identify outlier **sensors**, we average across the **epoch** dimension of our indicator matrix $C$ and obtain $C^{\\mu_e} \\in \\mathbb{R}^{S_\\mathcal{G}}$, which is a vector of fractional numbers $c^{\\mu_e}_i$ representing the percentage of **epochs** for which that sensor is an outlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_outliers = outlier_mask.astype(float).mean('epoch')\n",
    "prop_outliers # porportion of epochs that sensor is an outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d) Identify noisy sensors part 2\n",
    "\n",
    "Next, we define a fractional threshold $\\tau^{p}$ ($p$ for percentile; default, ``.20``) as a cutoff point for determining if a sensor should be marked artifactual. The sensor $i$ is flagged as noisy if $c^{\\mu_e}_i > \\tau^{p}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "prop_outliers[prop_outliers > threshold] # FLAGGED SENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the outlier channels to our flags\n",
    "bad_chs = prop_outliers[prop_outliers > threshold].coords.to_index().values # FLAGGED SENSORS\n",
    "pipeline.flags[\"ch\"].add_flag_cat(kind='ch_sd',\n",
    "                                  bad_ch_names=bad_chs)\n",
    "print(pipeline.flags['ch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Flag Noisy Epochs\n",
    "\n",
    "#### This step closely resembles the \"Flag Noisy Channels\" step. For the sake of brevity we will be more concise in the documentation.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/Flag_noisy_epochs.png\" alt=\"Flag noisy epochs\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## step 2a) standard deviation across the samples dimension $T$\n",
    "\n",
    "\n",
    "### **Take a moment below to notice** that the sensors flagged in step 1 are not in ``epochs_xr``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = pipeline.get_epochs()\n",
    "# Let's make our epochs array into a named Array\n",
    "epochs_xr = epochs_to_xr(epochs, kind='ch')\n",
    "trim_ch_sd = epochs_xr.std(\"time\")\n",
    "trim_ch_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a) part 2: We compute $\\sigma^{X^{\\sigma_{t}}[50,70]e}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " 50th and 75th quantile across epochs and the UQR\n",
    "\n",
    "<br>\n",
    "\n",
    "Like before, We Take the median and 70th quantile, but now we operate across **epochs**, resulting in two 1D vector's of size ``n_good_sensors`` $S_\\mathcal{G}$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $ X^{{\\sigma}_t{Q50^e}} = Q50^e(X^{\\sigma_{t}}) \\in \\mathbb{R}^{S_\\mathcal{G}} $\n",
    "- $ X^{{\\sigma}_t{Q75^e}} = Q75^e(X^{\\sigma_{t}}) \\in \\mathbb{R}^{S_\\mathcal{G}} $\n",
    "\n",
    "<br>\n",
    "\n",
    "$ UQR^e = (X^{{\\sigma}_T{Q75}^e} - X^{{\\sigma}_T{Q50}^e})$\n",
    "\n",
    "<br>\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "## Step 2b: define sensor-specific thresholds for rejecting epochs $\\tau^e_i$\n",
    "\n",
    "Our sensor-specifc threshold for rejecting epochs is defined by:\n",
    "\n",
    "\n",
    "$ \\tau^e_i = X^{{\\sigma}_T{Q50}^e} + UQR^e\\times k$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is step 2a\n",
    "mid_val, upper_val = trim_ch_sd.quantile([.5, .75], dim='epoch')\n",
    "upper_dist = upper_val - mid_val\n",
    "upper_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is step 2b\n",
    "k = 3\n",
    "u_out =  mid_val + upper_dist * k # sensor-specific thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c) Identify Noisy Epochs part 1\n",
    "\n",
    "<br>\n",
    "\n",
    "#### The indicator matrix is defined by\n",
    "\n",
    "$$\n",
    "c_{ij} =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } x^{\\sigma_{t}}_{ij} < \\tau^e_i \\\\\n",
    "1 & \\text{if } x^{\\sigma_{t}}_{ij} \\geq \\tau^e_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To identify outlier **epochs**, we average across the **sensor** dimension of our indicator matrix $C$ and obtain $C^{\\mu_s} \\in \\mathbb{R}^{E_\\mathcal{G}}$, which is a vector of fractional numbers $c^{\\mu_s}_j$ representing the percentage of **sensors** for which that epoch is an outlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is indicator matrix C\n",
    "outlier_mask = trim_ch_sd > u_out\n",
    "outlier_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Step 2c\n",
    "prop_outliers = outlier_mask.astype(float).mean('ch')\n",
    "prop_outliers = prop_outliers.drop_vars('quantile')\n",
    "prop_outliers  # percent of sensors that are bad during each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2d: Identify Noisy Epochs Part 2\n",
    "\n",
    "Next, we define a fractional threshold $\\tau^{p}$ ($p$ for percentile; default, ``.20``) as a cutoff point for determining if a epoch should be marked artifactual. The epoch $j$ is flagged as noisy if $c^{\\mu_s}_j > \\tau^{p}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "prop_outliers[prop_outliers > threshold] # FLAGGED EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the outlier epochs to our flags\n",
    "bad_epochs = prop_outliers[prop_outliers > threshold].coords.to_index().values # FLAGGED Epochs\n",
    "pipeline.flags[\"epoch\"].add_flag_cat(kind='ch_sd',\n",
    "                                  bad_epoch_inds=bad_epochs,\n",
    "                                     epochs=epochs)\n",
    "print(pipeline.flags['epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "# **Step 3**: Find Nearest Neighbours & return Maximum Correlation\n",
    "\n",
    "\n",
    "*Wheras steps 1 and 2 operated on a 2D matrix of standard deviation values, Steps 5, 6, 7, and 8 will operate on correlation coefficients. This Step describes the procedure for defining the 2D matrix of correlation coefficients.*\n",
    "\n",
    "<br>\n",
    "\n",
    "![Get nearest neighbours](https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/Nearest_neighbors.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylossless.pipeline import chan_neighbour_r\n",
    "# Posting full function definition for easier inspection\n",
    "chan_neighbour_r??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Take a moment to notice** that the flagged epochs from the previous step are dropped during epoching.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that our flagged epochs are dropped.\n",
    "epochs = pipeline.get_epochs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 3, part 1: Calculate Correlation Coefficents between each Sensor and it's nearest Neighbors\n",
    "\n",
    "<br>\n",
    "\n",
    "- For each good sensor $i$ in $S_{\\mathcal{G}}$, we select its $N$ nearest neighbors. I.e. the $N$ sensors that are closest to it.\n",
    "\n",
    "- We call the sensor $i$ the *origin*, and its nearest neighbors $\\hat{s_l}$ with $l \\in \\{1, 2, \\ldots, N\\}$\n",
    "\n",
    "- Then, for each epoch $j$, we calculate the correlation coefficient $\\rho^t_{(i,\\hat{s_l}),j}$ between origin sensor $i$ and each neighbor $\\hat{s_l}$ across dimension $t$ (samples):\n",
    "\n",
    "Returning a 3D matrice of correlation coefficients\n",
    "\n",
    "$$\n",
    "\\mathrm{P}^t = \\{\\rho^t_{(i, \\hat{s_l}),j}\\} \\in \\mathbb{R}^{S_G \\times E_G \\times n}\n",
    "$$\n",
    "\n",
    "Finally, we select the maximum correlation coefficient across the neighbor dimension $n$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathrm{P}^{t,{\\text{max}}^n}= \\max\\limits_{\\hat{s_l}}  \\rho^t_{(i, \\hat{s_l}),j}\n",
    "$$\n",
    "\n",
    "\n",
    "Returning a 2D matrix where each value at $(i, j)$ is the maximum correlation coefficient between sensor $i$ and its $N$ nearest neighbors, for epoch $j$\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r_ch = chan_neighbour_r(epochs, nneigbr=3, method='max')\n",
    "# maximum correlation out of correlations between ch and its 3 neighbors\n",
    "data_r_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------------\n",
    "\n",
    "<br>\n",
    "\n",
    "### This matrix $\\mathrm{P}^{t,{\\text{max}}^n}$  will be used in steps 5, 6, 7, and 8 below.\n",
    "\n",
    "<br>\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 5**: Flag Bridged Sensors\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/Flag_bridged_sensors.png\" width=\"50%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "---------------------\n",
    "\n",
    "## <center>Operations<center>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6a) Take the 25th, 50th, and 75th quantile across the epochs dimension of $\\mathrm{P}^{t,{\\text{max}}^n}$\n",
    "\n",
    "#### *and calculate the Inter Quantile Range (IQR)*\n",
    "\n",
    "\n",
    "$$\n",
    "IQR^e = \\mathrm{P}^{t,{\\text{max}}^nQ75^e} - \\mathrm{P}^{t,{\\text{max}}^nQ25^e}\n",
    "$$\n",
    "\n",
    "For each sensor, divide the median across epochs by the IQR across epochs. bridged channels should have a high median correlation but a low IQR of the correlation. We call this measure the bridge-indicator.\n",
    "\n",
    "$$\n",
    "\\mathcal{B}_s = \\frac{\\mathrm{P}^{t,{\\text{max}}^nQ50^e}}{IQR^e}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "-------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Define a bridging threshold\n",
    "\n",
    "Take the 25th, 50th, and 75th quantile of $\\mathcal{B}_s$\n",
    "\n",
    "And calculate the Inter Quantile Range* $IQR^s$. A channel $i$ is bridged if $\\mathcal{B}_i > B^{Q50^s} +k \\times IQR^s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from functools import partial\n",
    "\n",
    "msr = data_r_ch.median(\"epoch\") / data_r_ch.reduce(scipy.stats.iqr, dim=\"epoch\")\n",
    "# msr is a 1D vector of size n_sensors\n",
    "config_trim = 40\n",
    "config_bridge_z = 6\n",
    "\n",
    "trim = config_trim\n",
    "if trim >= 1:\n",
    "        trim /= 100\n",
    "trim /= 2 # .20 and will be used as (.20, .20)\n",
    "\n",
    "trim_mean = partial(scipy.stats.mstats.trimmed_mean,\n",
    "                        limits=(trim, trim))\n",
    "trim_std = partial(scipy.stats.mstats.trimmed_std,\n",
    "                        limits=(trim, trim))\n",
    "\n",
    "z_val = config_bridge_z  # 6\n",
    "mask = (msr > msr.reduce(trim_mean, dim=\"ch\")\n",
    "        + z_val*msr.reduce(trim_std, dim=\"ch\")) # bridged chans\n",
    "\n",
    "bridged_ch_names = data_r_ch.ch.values[mask]\n",
    "bridged_ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the outlier channels to our flags\n",
    "bad_chs = bridged_ch_names\n",
    "pipeline.flags[\"ch\"].add_flag_cat(kind='bridge',\n",
    "                                  bad_ch_names=bad_chs)\n",
    "pipeline.flags['ch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Flag Rank Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rank sensor $i$ is simply the sensor with the highest median (across epochs) correlation coefficient, out of the remaining set of good sensors, i.e.,\n",
    "\n",
    "$$\n",
    "i = \\text{arg}\\max\\limits_i \\rho_{j}^{t,{\\text{max}}^n,median^j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_chs = [ch for ch in data_r_ch.ch.values\n",
    "          if ch not in pipeline.flags[\"ch\"].get_flagged()]\n",
    "data_r_ch_good = data_r_ch.sel(ch=good_chs)\n",
    "\n",
    "flag_ch = [str(data_r_ch_good.median(\"epoch\")\n",
    "                                     .idxmax(dim=\"ch\")\n",
    "                                     .to_numpy()\n",
    "                            )]\n",
    "pipeline.flags['ch'].add_flag_cat(kind='rank', bad_ch_names=flag_ch)\n",
    "pipeline.flags['ch']['rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------------------------------\n",
    "\n",
    "# Step 7: Flag low correlation Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This step closely follows Step 1 except we use the lower end of the distribution to set the outliers threshold.\n",
    "\n",
    "- We will be succinct in the documentation. Please Feel free to inspect the variables in the python code as needed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------\n",
    "\n",
    "## <center>Operations<center>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6a) Take the 30th and 50th quantile of $\\mathrm{P}^{t,{\\text{max}}^n}$ across the sensors dimension\n",
    "\n",
    "*and calculate the Lower Quantile Range* $LQR^s$\n",
    "\n",
    "Resulting in vectors $\\mathrm{P}^{t,{\\text{max}}^nQ25^s}$ and $\\mathrm{P}^{t,{\\text{max}}^nQ50^s}$ of size $E_\\mathcal{G}$. The lower quantile range ($LQR$) is defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "LQR^s = \\mathrm{P}^{t,{\\text{max}}^nQ50^s} - \\mathrm{P}^{t,{\\text{max}}^nQ25^s}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6b) Define epoch-specific thresholds for rejecting sensors\n",
    "\n",
    "$\\tau^s_j$ represents our set of epoch-specific thresholds:\n",
    "\n",
    "\n",
    "$$\n",
    "\\tau^s = \\mathrm{P}^{t,{\\text{max}}^nQ50^s} - LQR^s\\times k\n",
    "$$\n",
    "\n",
    "Now, we compare our 2D correlation coefficient matrix $\\mathrm{P}^{t,{\\text{max}}^n} = \\{ \\rho^{t,{\\text{max}}^n}_{ij} \\}$ to the threshold vector $\\tau^s_j$ resulting in the indicator matrix $C \\in \\{0, 1\\}^{S \\times E}=\\{c_{ij}\\}$ with\n",
    "\n",
    "$$\n",
    "c_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\rho^{t,{\\text{max}}^n}_{ij} \\leq \\tau^s_j \\\\\n",
    "0 & \\text{if } \\rho^{t,{\\text{max}}^n}_{ij} > \\tau^s_j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c) Identify uncorrelated sensors\n",
    "\n",
    "To identify uncorrelated **sensors**, we average across the **epoch** dimension of our indicator matrix $C$ and obtain $C^{\\mu_e} \\in \\mathbb{R}^{S_\\mathcal{G}}$, which is a vector of fractional numbers $c^{\\mu_e}_i$ representing the percentage of **epochs** for which that sensor is an outlier.\n",
    "\n",
    "\n",
    "Next, we define a fractional threshold $\\tau^{p}$ ($p$ for percentile; default, ``.20``) as a cutoff point for determining if a sensor should be marked artifactual. The sensor $i$ is flagged as noisy if $c^{\\mu_e}_i > \\tau^{p}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step a\n",
    "lower_val, mid_val = data_r_ch.quantile([.3, .5], dim='ch')\n",
    "\n",
    "# step B. Define the threshold\n",
    "lower_dist = mid_val - lower_val\n",
    "\n",
    "# Step B\n",
    "k = 3\n",
    "l_out =  mid_val - lower_dist * k\n",
    "\n",
    "# Step D\n",
    "outlier_mask = data_r_ch < l_out\n",
    "\n",
    "prop_outliers = outlier_mask.astype(float).mean('epoch')\n",
    "prop_outliers = prop_outliers.drop_vars('quantile')\n",
    "\n",
    "threshold = 0.2\n",
    "prop_outliers[prop_outliers > threshold] # FLAGGED SENSORS\n",
    "\n",
    "# Let's add the outlier channels to our flags\n",
    "bad_chs = prop_outliers[prop_outliers > threshold].coords.to_index().values # FLAGGED SENSORS\n",
    "pipeline.flags[\"ch\"].add_flag_cat(kind='uncorrelated',\n",
    "                                  bad_ch_names=bad_chs)\n",
    "print(pipeline.flags['ch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------\n",
    "\n",
    "<br>\n",
    "\n",
    "# Step 8: Flag Uncorrelated Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is similar to step 6 (flag uncorrelated channels) and step 2 (flag noisy epochs). For the sake of brevity we will not re-describe each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------------------------\n",
    "\n",
    "## <center>Operations<center>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6a) Take $LQR^e$ of $\\mathrm{P}^{t,{\\text{max}}^n}$\n",
    "\n",
    "\n",
    "### 6b) Defined sensor-specific thresholds\n",
    "\n",
    "We get the indicator matrix as described previously, using\n",
    "\n",
    "$$\n",
    " \\tau^e_i = \\mathrm{P}^{t,{\\text{max}}^nQ50^e} - LQR^e\\times k\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "c_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\rho^{t,{\\text{max}}^n}_{ij} < \\tau^e_i \\\\\n",
    "0 & \\text{if } \\rho^{t,{\\text{max}}^n}_{ij} \\geq \\tau^e_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6c) Identify uncorrelated sensors\n",
    "<br>\n",
    "\n",
    "We define a fractional threshold as we did in the previous step and flag uncorrelated epochs $j$ if $c^{\\mu_s}_j > \\tau^{p}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step a\n",
    "lower_val, mid_val = data_r_ch.quantile([.3, .5], dim='epoch')\n",
    "\n",
    "# step B. Define the threshold\n",
    "lower_dist = mid_val - lower_val\n",
    "\n",
    "# Step B\n",
    "k = 3\n",
    "l_out =  mid_val - lower_dist * k\n",
    "\n",
    "# Step D\n",
    "outlier_mask = data_r_ch < l_out\n",
    "\n",
    "prop_outliers = outlier_mask.astype(float).mean('ch')\n",
    "prop_outliers = prop_outliers.drop_vars('quantile')\n",
    "\n",
    "threshold = 0.2\n",
    "prop_outliers[prop_outliers < threshold] # FLAGGED EPOCHS\n",
    "\n",
    "# Let's add the outlier epochs to our flags\n",
    "bad_epochs = prop_outliers[prop_outliers > threshold].coords.to_index().values # FLAGGED EPOCHS\n",
    "pipeline.flags[\"epoch\"].add_flag_cat(kind='uncorrelated',\n",
    "                                  bad_epoch_inds=bad_epochs,\n",
    "                                     epochs=epochs)\n",
    "print(pipeline.flags['ch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Run Initial ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run_ica('run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Identify outlying ICA Time courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step follows exactly step 1 so we will not re-describe the operations here.\n",
    "\n",
    "There are however a few key differences:\n",
    "\n",
    "- The input 3D matrix is ``(n_components, n_good_epochs, n_times)``: $ X \\in \\mathbb{R}^{C\\times E_G \\times{T}} $\n",
    "- the default threshold $k$ is 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.flags[\"ic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Run Final ICA\n",
    "\n",
    "# This is the Final ICA, excluding all flagged epochs and channels.\n",
    "\n",
    "# The ``ICLabel`` Neural Network will be run on this ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run_ica('run2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Find outlying channels to leave out of Re Reference\n",
    "\n",
    "- This robust average reference is performed before the first pipeline step and between most pipeline steps.\n",
    "\n",
    "- It does not flag sensors but identifies outlier sensors and leaves them out of the reference\n",
    "\n",
    "Like steps 1 and 2, this step starts with 3D matrix $X \\in \\mathbb{R}^{S \\times E \\times T}$ and takes the standard deviation across axis $T$:\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/robust_rereference.png\" alt=\"Find outliers and leave out of rereference\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "---------------------\n",
    "\n",
    "## <center>Operations<center>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6a) Take the median, 30th, and 70th quantile across dimension channels of  $X^{\\sigma_{t}} \\in \\mathbb{R}^{S \\times E}$, and Calculate the Inter Quantile Range (IQR).\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Create a non-parametric Z score\n",
    "\n",
    "$$\n",
    "\\mathcal{Z}^{\\sigma^{t}} = \\frac{X^{\\sigma^{t}Q50^s} - X^{\\sigma^{t}}}{IQR^s}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### 6b) Define  threshold\n",
    "\n",
    "Sensor $i$ is left out of the re-reference if\n",
    "\n",
    "$$\n",
    "\\mathcal{Z}^{\\sigma^{t}\\mu^{e}} > \\mathcal{Z}^{\\sigma^{t}\\mu^{e}Q50^s} + k \\times IQR^s\n",
    "$$\n",
    "\n",
    "with $IQR^s = \\mathcal{Z}^{\\sigma^{t}\\mu^{e}Q75^s}-\\mathcal{Z}^{\\sigma^{t}\\mu^{e}Q25^s}$ and where $\\mathcal{Z}^{\\sigma^{t}\\mu^{e}}$ is the mean of $\\mathcal{Z}^{\\sigma^{t}}$ across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is step 2a\n",
    "ch_dist = trim_ch_sd - trim_ch_sd.median(dim=\"ch\")\n",
    "perc_30 = trim_ch_sd.quantile(0.3, dim=\"ch\")\n",
    "perc_70 = trim_ch_sd.quantile(0.7, dim=\"ch\")\n",
    "\n",
    "# This is step b: non-parametric z-score\n",
    "ch_dist /= perc_70 - perc_30  # shape (chans, epoch)\n",
    "\n",
    "# This is step c\n",
    "mean_ch_dist = ch_dist.mean(dim=\"epoch\")\n",
    "\n",
    "# Step D begins here\n",
    "# find the median and 30 and 70 percentiles\n",
    "# of the mean of the channel distributions\n",
    "mdn = np.median(mean_ch_dist)\n",
    "deviation = np.diff(np.quantile(mean_ch_dist, [0.3, 0.7]))\n",
    "print(f\"median: {mdn}, std: {deviation}\")\n",
    "\n",
    "# Thresholding. True sensors are left out\n",
    "mean_ch_dist > mdn+6*deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are any sensor names that are excluded\n",
    "mean_ch_dist.ch[mean_ch_dist > mdn+6*deviation].values.tolist()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
