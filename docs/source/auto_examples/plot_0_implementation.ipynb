{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# PyLossless Algorithms\n\nThis tutorial explains the calculations that PyLossless performs at each step of the\npipeline. We will use example EEG data to demonstrate the\ncalculations.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can open this notebook in\n    [Google Colab](https://colab.research.google.com/drive/1ecyNo10oFgpbVNuD7Ztgr2fs8XkpfOYo?usp=sharing)!</p></div>\n\n\n## Notation \nBefore we begin, we define some notation that will be used throughout the text:\n\n- We start with a 3D matrix of EEG data,\n  $X \\in \\mathbb{R}^{S_\\mathcal{G} \\times E_\\mathcal{G} \\times T}$,\n  where $S_\\mathcal{G}$ and $E_\\mathcal{G}$ are the sets of good sensors and\n  epochs, respectively, and $T$, is the number of samples(i.e. time-points).\n\n- ``s``, ``e``, and ``t`` are sensor, epochs, and samples, respectively.\n\n- We use superscripts to denote operations across a dimension, and we use subscripts to\n  denote indexing a dimension.\n\n- We refer to a single sensor $i$ as\n  $X\\big|_{s=i} \\in \\mathbb{R}^{E_\\mathcal{G} \\times T}$,\n  with $i \\in S_\\mathcal{G}$.\n\n- We refer to a single epoch $j$ as\n  $X\\big|_{e=j} \\in \\mathbb{R}^{S_\\mathcal{G} \\times T}$,\n  with $j \\in E_\\mathcal{G}$.\n\n- We denote sensor-specific thresholds for rejecting epochs as\n  $\\tau^e_i \\in \\mathbb{R}^{S_\\mathcal{G}}$\n\n- We denote epoch-specific thresholds for rejecting sensors as\n  $\\tau^s_j \\in \\mathbb{R}^{E_\\mathcal{G}}$\n\n- We denote *quantiles* as $Q\\#^{dim}$: i.e. $Q75^s$ is the 75th *quantile*\n  along the sensor dimension. The function $Q75^s(X)$ computes the 75th quantile\n  along the $s$ dimension of matrix $X$, resulting in a matrix noted\n  $X^{Q75^s} \\in \\mathbb{R}^{E \\times T}$.\n\nThroughout the text, we use capital letters for matrices and lowercase letters for\nscalars. For example, the data point for sensor $i$, epoch $j$, and\ntime $k$ is denoted as $X\\big|_{s=i; e=j; t=k} = x_{ijk} \\in \\mathbb{R}$,\nand $X=\\{x_{ijk}\\}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and data loading\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport numpy as np\n\nimport mne\nfrom mne.datasets import sample\n\nimport pylossless as ll\n\n# Load example mne data\nraw = ll.datasets.load_simulated_raw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a PyLossless configuration file\nLet's load a PyLossless configuration file. This file contains the parameters that\nwill be used for each step of the pipeline. For example, the ``ch_ch_sd`` section\ncontains the parameters for the `noisy_sensors` step. We can modify these\nparameters to change the behavior of the pipeline. For example, we can change the\npercent of epochs that a sensor must be noisy for it to be flagged via the\n``flag_crit`` parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "config = ll.config.Config()\nconfig.load_default()\nconfig[\"ch_ch_sd\"][\"outliers_kwargs\"][\"lower\"] = 0.25  # lower quantile\nconfig[\"ch_ch_sd\"][\"outliers_kwargs\"][\"upper\"] = 0.75  # upper quantile\nconfig[\"ch_ch_sd\"][\"flag_crit\"] = 0.30  # percent of epochs that a sensor must be noisy\nconfig.save(\"test_config.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a pipeline instance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline = ll.LosslessPipeline(\"test_config.yaml\")\npipeline.raw = raw\nraw.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input Data\n\nFirst, we epoch the data to be used for subsequent steps.\nLet our 3D matrix below be defined as $X \\in \\mathbb{R}^{S \\times E \\times T}$\nwhere $X$ is a matrix of real numbers and of dimension $S$ sensors\n$\\times$ E$ epochs `\\times T` times.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = pipeline.get_epochs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's convert our epochs object into a named :class:`xarray.DataArray` object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pylossless.pipeline import epochs_to_xr\n\n#\nepochs_xr = epochs_to_xr(epochs, kind=\"ch\")\nepochs_xr  # 277 epochs, 50 sensors, 602 samples per epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Robust Average Reference\n\n.. figure:: https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/robust_rereference.png\n   :align: center\n   :alt: Robust Average Reference graphic.\n\n   Robust Average Reference. The figure shows the steps for robust average referencing.\n   See the text below for descriptions of mathematical notation.\n\nBefore the pipeline can begin, we must average reference the data. This is because\nthe pipeline uses data distributions to identify noisy sensors, and For EEG data that\nuses an online reference to a single electrode, sensors that are further from the\nreference will have a higher voltage variance, and the pipeline will be biased to\nflag these sensors as noisy. The average reference, which subtracts the average\nsignal across sensors from each individual sensor, will ensure an even playing field.\nHoweer, we dont want to include noisy sensors in the average reference signal. So we\nwill identify noisy sensors and and leave them out of the average reference signal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_std = epochs_xr.std(\"time\")\nq25_ch = sample_std.quantile(0.25, dim=\"ch\")\nq50_ch = sample_std.median(dim=\"ch\")\nq75_ch = sample_std.quantile(0.75, dim=\"ch\")\nch_dist = sample_std - q50_ch  # center the data\nch_dist /= q75_ch - q25_ch  # shape (chans, epoch)\n\nmean_ch_dist = ch_dist.mean(dim=\"epoch\")  # shape (chans)\n\n# find the median and 25 and 75 percentiles\n# of the mean of the channel distributions\nmdn = np.median(mean_ch_dist)\ndeviation = np.diff(np.quantile(mean_ch_dist, [0.25, 0.75]))\n\nleave_out = mean_ch_dist.ch[mean_ch_dist > mdn + 6 * deviation].values.tolist()\nleave_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref_chans = [ch for ch in epochs.pick(\"eeg\").ch_names if ch not in leave_out]\npipeline.raw.set_eeg_reference(ref_channels=ref_chans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Flag Noisy Sensors\n.. figure:: https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/Flag_noisy_sensors.png\n   :align: center\n   :alt: Flag Noisy Sensors graphic.\n\n   Flag Noisy Sensors. The figure shows the steps for flagging noisy sensors. See the text below\n   for descriptions of mathematical notation.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we applied a robust average reference to the raw data, we will need to re-epoch\nthe data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = pipeline.get_epochs()\nepochs_xr = epochs_to_xr(epochs, kind=\"ch\")\n\n# First we take standard deviation of\n# :math:`X \\in \\mathbb{R}^{S \\times E \\times T}` across the samples dimension :math:`t`\n# resulting in a 2D matrix :math:`X^{\\sigma_{t}} \\in \\mathbb{R}^{S \\times E}`\ntrim_ch_sd = epochs_xr.std(\"time\")\ntrim_ch_sd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### a) Take the 50th and 75th quantile across dimension sensor of $X^{\\sigma_{t}}$\n\nThis operation results in two 1D vectors of size $E$:\n\n\\begin{align}X^{{\\sigma}_t{Q50^s}} = Q50^s(X^{\\sigma_{t}}) \\in \\mathbb{R}^{E}\\end{align}\n\\begin{align}X^{{\\sigma}_t{Q75^s}} = Q75^s(X^{\\sigma_{t}}) \\in \\mathbb{R}^{E}\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "q50, q75 = trim_ch_sd.quantile([0.5, 0.75], dim=\"ch\")\nq50  # a 1D array of median standard deviation values across channels for each epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### b) Define an Upper Quantile Range as $Q75 - Q50$\n\n\\begin{align}UQR^s = X^{{\\sigma}_T{Q75}^s} - X^{{\\sigma}_T{Q50}^s}\\end{align}\n\nThis operation results in a 1D vector of size $E$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "uqr = q75 - q50\nuqr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c) Identify outlier Indices $(i, j)$\n\nWe multiply a constant $k$ by the $UQR$ to define a measure for the\nspread of the right tail of the distribution of $X^{\\sigma_{t}}$ values and\nadd it to the median of $X^{\\sigma_{t}}$ to obtain epoch-specific standard\ndeviation threshold for outliers:\n\n\\begin{align}\\tau^s_j = X^{{\\sigma}_T{Q50}^S} + UQR^s\\times k\\end{align}\n\nThat is, $\\tau^s_j$ is the epoch-specific threshold for the epoch $j$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "k = 3\nupper_threshold = q50 + q75 * k\nupper_threshold  # epoch specific thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we compare our 2D standard deviation matrix to the threshold vector of\n$\\tau^e_j$:\n\n\\begin{align}X^{\\sigma_{t}} \\big|_{e=j}  > \\tau^s_j\\end{align}\n\nresulting in the indicator matrix $C \\in \\{0, 1\\}^{S \\times E}=\\{c_{ij}\\}$:\n\n\\begin{align}c_{ij} =\n   \\begin{cases}\n   0 & \\text{if } x^{\\sigma_{t}}_{ij} < \\tau^s_j \\\\\n   1 & \\text{if } x^{\\sigma_{t}}_{ij} \\geq \\tau^s_j\n   \\end{cases}\\end{align}\n\nEach element of this matrix indicates whether sensor $i$ is an outlier at an epoch\n$j$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "outlier_mask = trim_ch_sd > upper_threshold\noutlier_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### d) Identify noisy sensors part 1\n\nTo identify outlier sensors, we average across the epoch dimension of our indicator\nmatrix $C$ and obtain $C^{\\mu_e} \\in \\mathbb{R}^{S_\\mathcal{G}}$, which\nis a vector of fractional numbers $c^{\\mu_e}_i$ representing the percentage of\nepochs for which that sensor is an outlier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "percent_outliers = outlier_mask.astype(float).mean(\"epoch\")\npercent_outliers  # percent of epochs that sensor is an outlier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### e) Identify noisy sensors part 2\n\nNext, we define a threshold $\\tau^{p}$ ($p$ for percentile;\ndefault, ``.20``) as a cutoff point for determining if a sensor should be marked\nartifactual. The sensor $i$ is flagged as noisy if\n$c^{\\mu_e}_i > \\tau^{p}$. That is, if the sensor is an outlier for more than\n$\\tau^{p}$ percent of the epochs, it is flagged as noisy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p_threshold = config[\"ch_ch_sd\"][\"flag_crit\"]  # 0.3, or 30%\nnoisy_chs = percent_outliers[percent_outliers > p_threshold].coords.to_index().values\nnoisy_chs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### f) Add the noisy sensors to the pipeline flags\n\nLet's add the noisy sensors to the pipeline flags.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.flags[\"ch\"].add_flag_cat(kind=\"ch_sd\", bad_ch_names=noisy_chs)\npipeline.raw.info[\"bads\"].extend(pipeline.flags[\"ch\"][\"ch_sd\"].tolist())\npipeline.flags[\"ch\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Flag Noisy Epochs\n\nThis step closely resembles the `noisy_sensors` step. For the sake of brevity\nwe will be more concise in the documentation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. figure:: https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/Flag_noisy_epochs.png\n   :align: center\n\n   Flag Noisy Epochs. The figure shows the steps for flagging noisy epochs. See the text below\n   for descriptions of mathematical notation.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### a) Take standard deviation across the samples dimension $t$\n\nTake a moment below to notice that the sensors flagged in the prior setp are not in\n``epochs_xr`` below:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = pipeline.get_epochs()\n# Let's make our epochs array into a named Array\nepochs_xr = epochs_to_xr(epochs, kind=\"ch\")\ntrim_ch_sd = epochs_xr.std(\"time\")\ntrim_ch_sd.coords[\"ch\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### b) Compute 50th and 75th quantile across epochs and the UQR\n\nLike before, We Take the median and 70th quantile, but now we operate across epochs,\nresulting in two 1D vector's of size ``n_good_sensors`` $S_\\mathcal{G}$\n\n\\begin{align}X^{{\\sigma}_t{Q50^e}} = Q50^e(X^{\\sigma_{t}}) \\in \\mathbb{R}^{S_\\mathcal{G}}\\end{align}\n\\begin{align}X^{{\\sigma}_t{Q75^e}} = Q75^e(X^{\\sigma_{t}}) \\in \\mathbb{R}^{S_\\mathcal{G}}\\end{align}\n\\begin{align}UQR^e = (X^{{\\sigma}_T{Q75}^e} - X^{{\\sigma}_T{Q50}^e})\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c) Define sensor-specific thresholds for rejecting epochs $\\tau^e_i$\n\nOur sensor-specifc threshold for rejecting epochs is defined by:\n\n\\begin{align}\\tau^e_i = X^{{\\sigma}_T{Q50}^e} + UQR^e\\times k\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "q50, q75 = trim_ch_sd.quantile([0.5, 0.75], dim=\"epoch\")\nuqr_epoch = q75 - q50\nuqr_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "k = 8\nupper_threshold = q50 + uqr_epoch * k\nupper_threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### d) Identify Outlier indices\n\nThe indicator matrix is defined by:\n\n\\begin{align}c_{ij} =\n   \\begin{cases}\n   0 & \\text{if } x^{\\sigma_{t}}_{ij} < \\tau^e_i \\\\\n   1 & \\text{if } x^{\\sigma_{t}}_{ij} \\geq \\tau^e_i\n   \\end{cases}\\end{align}\n\n\nTo identify outlier **epochs**, we average across the **sensor** dimension of our\nindicator matrix $C$ and obtain\n$C^{\\mu_s} \\in \\mathbb{R}^{E_\\mathcal{G}}$, which is a vector of numbers\n$c^{\\mu_s}_j$ representing the percentage of **sensors** for which that epoch\nis an outlier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "outlier_mask = trim_ch_sd > upper_threshold\noutlier_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "percent_outliers = outlier_mask.astype(float).mean(\"ch\")\npercent_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### e) Identify noisy epochs\n\nNext, we define a fractional threshold $\\tau^{p}$ as a cutoff point for\ndetermining if a epoch should be marked artifactual. The epoch $j$ is flagged\nas noisy if $c^{\\mu_s}_j > \\tau^{p}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bad_epochs = percent_outliers[percent_outliers > p_threshold].coords.to_index().values\nbad_epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### f) Add the noisy epochs to the pipeline flags\n\nLet's add the outlier epochs to our flags\nThese will be added directly as :class:`mne.Annotations` to the raw data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.flags[\"epoch\"].add_flag_cat(\n    kind=\"ch_sd\", bad_epoch_inds=bad_epochs, epochs=epochs\n)\npipeline.raw.annotations.description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.raw.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filtering\n\nAfter flagging noisy sensors and epochs, we filter the data. By default,\nThe pipeline uses a 1-100Hz bandpass filter. This is because 1), ICA decompositions\nare more stable when low frequency drifts are removed, and 2) the ICLabel classifier\nis trained on data that has been filtered between 1-100Hz. A notch filter can also be\noptionally specified.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.config[\"filtering\"][\"notch_filter_args\"][\"freqs\"] = [50]\npipeline.filter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find Nearest Neighbours & return Maximum Correlation\n\n.. figure:: https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/Nearest_neighbors.png\n   :align: center\n   :alt: Nearest Neighbors graphic.\n\n   Nearest Neighbors. The figure shows the steps for finding nearest neighbors. See the text below\n   for descriptions of mathematical notation.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Whereas `noisy_sensors` and `noisy_epochs` operated on a 2D matrix of\nstandard deviation values, The next few steps will operate on correlation\ncoefficients. Here we describe the procedure for defining the 2D matrix of correlation\ncoefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pylossless.pipeline import chan_neighbour_r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that our flagged epochs are dropped.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = pipeline.get_epochs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### a) Calculate Correlation Coefficients between each Sensor and its neighboring eighbors\n\n- For each good sensor $i$ in $S_{\\mathcal{G}}$, we select its $N$ nearest\n  neighbors. I.e. the $N$ sensors that are closest to it.\n\n- We call the sensor $i$ the *origin*, and its nearest neighbors :math`\\hat{s_l}`\n  with $l \\in \\{1, 2, \\ldots, N\\}$\n\n- Then, for each epoch $j$, we calculate the correlation coefficient\n  $\\rho^t_{(i,\\hat{s_l}),j}$ between origin sensor $i$ and each neighbor\n  $\\hat{s_l}$ across dimension $t$ (samples), returning a 3D matrice of\n  correlation coefficients:\n\n\\begin{align}\\mathrm{P}^t = \\{\\rho^t_{(i, \\hat{s_l}),j}\\} \\in \\mathbb{R}^{S_G \\times E_G \\times n}\\end{align}\n\nFinally, we select the maximum correlation coefficient across the neighbor dimension\n$n$:\n\n\\begin{align}\\mathrm{P}^{t,{\\text{max}}^n}= \\max\\limits_{\\hat{s_l}}  \\rho^t_{(i, \\hat{s_l}),j}\\end{align}\n\nReturning a 2D matrix where each value at $(i, j)$ is the maximum correlation\ncoefficient between sensor $i$ and its $N$ nearest neighbors, at each epoch\n$j$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_r_ch = chan_neighbour_r(epochs, nneigbr=3, method=\"max\")\n# maximum correlation out of correlations between ch and its 3 neighbors\ndata_r_ch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This matrix $\\mathrm{P}^{t,{\\text{max}}^n}$  will be used in the steps below.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flag Bridged Sensors\n\n.. figure:: https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/Flag_bridged_sensors.png\n   :align: center\n   :alt: Flag Bridged Sensors graphic.\n\n   Flag Bridged Sensors. The figure shows the steps for flagging bridged sensors.\n   See the text below for descriptions of mathematical notation.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### a) Calculate the 50th, 75th quantile and IQR across epochs\n\n\\begin{align}IQR^e = \\mathrm{P}^{t,{\\text{max}}^nQ75^e} - \\mathrm{P}^{t,{\\text{max}}^nQ25^e}\\end{align}\n\nFor each sensor, divide the median across epochs by the IQR across epochs. Bridged\nchannels should have a high median correlation but a low IQR of the correlation.\nWe call this measure the bridge-indicator.\n\n\\begin{align}\\mathcal{B}_s = \\frac{\\mathrm{P}^{t,{\\text{max}}^nQ50^e}}{IQR^e}\\end{align}\n\n### b) Define a bridging threshold\nNow, take the 25th, 50th, and 75th quantile of $\\mathcal{B}_s$ across sensors,\nAnd calculate the $IQR^s$. A channel $i$ is bridged if\n\n\\begin{align}\\mathcal{B}_i > B^{Q50^s} +k \\times IQR^s\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import scipy\nfrom functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "msr = data_r_ch.median(\"epoch\") / data_r_ch.reduce(scipy.stats.iqr, dim=\"epoch\")\n# msr is a 1D vector of size n_sensors\nconfig_trim = 40\nconfig_bridge_z = 6\n#\ntrim = config_trim\nif trim >= 1:\n    trim /= 100\ntrim /= 2  # .20 and will be used as (.20, .20)\n#\ntrim_mean = partial(scipy.stats.mstats.trimmed_mean, limits=(trim, trim))\ntrim_std = partial(scipy.stats.mstats.trimmed_std, limits=(trim, trim))\n#\nz_val = config_bridge_z  # 6\nmask = msr > msr.reduce(trim_mean, dim=\"ch\") + z_val * msr.reduce(\n    trim_std, dim=\"ch\"\n)  # bridged chans\n#\nbridged_ch_names = data_r_ch.ch.values[mask]\nbridged_ch_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's add the outlier channels to our flags\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bad_chs = bridged_ch_names\npipeline.flags[\"ch\"].add_flag_cat(kind=\"bridge\", bad_ch_names=bad_chs)\npipeline.flags[\"ch\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identify the Rank Channel\n\nBecause the pipeline uses an average reference before the ICA decomposition, it is\nnecessary to account for rank deficiency (i.e., every sensor in the montage is\nlinearly dependent on the other channels due to the common average reference). To\naccount for this, the pipeline flags the sensor (out of the remaining good sensors)\nwith the highest median of the max correlation coefficient with its neighbors\n(across epochs):\n\n\\begin{align}\\begin{equation}\n   i = \\text{arg}\\max\\limits_i \\rho_{i}^{t,{\\text{max}}^n,median^j}\n   \\end{equation}\\end{align}\n\nThis sensor has the least unique time-series out of the remaining set of good sensors\n$S_\\mathcal{G}$ and is flagged by the pipeline as ``\u201drank\u201d``. Note that this\nsensor is not flagged because it contains artifact, but only because one of the\nremaining sensors needs to be removed to address rank deficiency before ICA\ndecomposition is performed. By choosing this sensor, we are likely to lose little\ninformation because of its high correlation with its neighbors. This sensor can be\nreintroduced after the ICA has been applied for artifact corrections.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "good_chs = [\n    ch for ch in data_r_ch.ch.values if ch not in pipeline.flags[\"ch\"].get_flagged()\n]\ndata_r_ch_good = data_r_ch.sel(ch=good_chs)\n\nflag_ch = [str(data_r_ch_good.median(\"epoch\").idxmax(dim=\"ch\").to_numpy())]\npipeline.flags[\"ch\"].add_flag_cat(kind=\"rank\", bad_ch_names=flag_ch)\npipeline.flags[\"ch\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flag low correlation Epochs\n\nThis step is designed to identify time periods in which many sensors are\nuncorrelated with neighboring sensors. It is similar to the `noisy_sensors` step,\n\nAgain we calculate the 25th and 50th quantile\nof $\\mathrm{P}^{t,{\\text{max}}^n}$, across the epochs dimension, and calculate\nthe lower quantile range $LQR^s$. This results in vectors\n$\\mathrm{P}^{t,{\\text{max}}^nQ25^e}$ and\n$\\mathrm{P}^{t,{\\text{max}}^nQ50^e}$ of size $S_\\mathcal{G}$. As for previous\nsteps, we define  sensor-specific thresholds for flagging epochs:\n\n\\begin{align}\\begin{equation}\n   \\tau^e = \\mathrm{P}^{t,{\\text{max}}^nQ50^e} - LQR^e\\times k\n   \\end{equation}\\end{align}\n\nAnd the corresponding indicator matrix:\n\n\\begin{align}\\begin{equation}\n   c_{ij} =\n   \\begin{cases}\n   1 & \\text{if } \\rho^{t,{\\text{max}}^n}_{ij} < \\tau^e_i \\\\\n   0 & \\text{if } \\rho^{t,{\\text{max}}^n}_{ij} \\geq \\tau^e_i\n   \\end{cases}\n   \\end{equation}\\end{align}\n\nWe average the indicator matrix across sensors and obtain a vector $C^{\\mu_s}$\nthat we use to flag uncorrelated epochs using the following criterion:\n\n\\begin{align}c^{\\mu_e}_i > \\tau^{p}.\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step a\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "q25, q50 = data_r_ch.quantile([0.25, 0.5], dim=\"epoch\")\n#\n# Define the LQR\nlqr = q50 - q25\n#\n# define a threshold\nk = 3\nlower_threshold = q50 - lqr * k\n#\noutlier_mask = data_r_ch < lower_threshold\n#\npercent_outliers = outlier_mask.astype(float).mean(\"ch\")\n#\np_threshold = 0.2\nbad_epochs = percent_outliers[percent_outliers > p_threshold].coords.to_index().values\n#\n# Add the outlier epochs to our flags\npipeline.flags[\"epoch\"].add_flag_cat(\n    kind=\"uncorrelated\", bad_epoch_inds=bad_epochs, epochs=epochs\n)\npipeline.raw.annotations.description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "in this case, no epochs were flagged as uncorrelated.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flag low correlation Sensors\n\n.. figure:: https://raw.githubusercontent.com/scott-huberty/wip_pipeline-figures/main/Flag_uncorrelated_sensors.png\n   :align: center\n   :alt: Flag Uncorrelated Sensors graphic.\n\n   Flag Uncorrelated Sensors. The figure shows the steps for flagging uncorrelated\n   sensors. See the text below for descriptions of mathematical notation.\n\nThis step is designed to identify sensors that have an unusually low correlation with\nneighboring sensors. The operations involved by this step are similar to those of the\n`noisy_sensors` step, except we use maximal nearest neighbor correlations instead\nof dispersion and the left instead of the right tail of the distribution to set\nthe threshold for outliers.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### a) Take lower quantile range and defined sensor-specific thresholds\n\nWe get the indicator matrix as described previously, using\n\n\\begin{align}\\tau^e_i = \\mathrm{P}^{t,{\\text{max}}^nQ50^e} - LQR^e\\times k\\end{align}\n\nand\n\n\\begin{align}c_{ij} =\n   \\begin{cases}\n   1 & \\text{if } \\rho^{t,{\\text{max}}^n}_{ij} < \\tau^e_i \\\\\n   0 & \\text{if } \\rho^{t,{\\text{max}}^n}_{ij} \\geq \\tau^e_i\n   \\end{cases}\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### b) Identify uncorrelated sensors\n\nWe define a threshold as we did in the previous step and flag uncorrelated epochs\n$j$ if $c^{\\mu_s}_j > \\tau^{p}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "q25, q50 = data_r_ch.quantile([0.25, 0.5], dim=\"ch\")\n#\n# Define LQR\nlqr = q50 - q25\n#\n# define a threshold\nk = 3\nlower_threshold = q50 - lqr * k\n#\n# Identify correlations less than the threshold\noutlier_mask = data_r_ch < lower_threshold\npercent_outliers = outlier_mask.astype(float).mean(\"epoch\")\n#\np_threshold = 0.2\nbad_chs = percent_outliers[percent_outliers > p_threshold].coords.to_index().values\n#\n# Add the outlier channels to our flags\npipeline.flags[\"ch\"].add_flag_cat(kind=\"uncorrelated\", bad_ch_names=bad_chs)\npipeline.flags[\"ch\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, no sensors were flagged as uncorrelated.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Initial ICA\n\nThe pipeline by runs ICA two times. The first ICA is only used to identify\nnoisy periods in its IC activation time-series. For this reason, the pipeline\nuses the FastICA algorithm for speed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.run_ica(\"run1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flag Noisy IC Activation time-periods\n\nThis step follows the same procedure as the `noisy_sensors` step, except that\nthe data is now the IC activation time-series. thus we start with a 3D matrix\n$X_{ica} \\in \\mathbb{R}^{I_\\mathcal{G} \\times E_\\mathcal{G} \\times T}$ of\nIC time-courses rather than scalp EEG data and where $I$ is the set of\nindependent components.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.flag_epoch_ic_sd1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.raw.annotations.description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Final ICA\n\nNow The pipeline runs the final ICA decomposition, this time using the extended\nInfomax algorithm. Note that any sensors or time-periods that have been flagged\nup to this point will not be passed into the ICA decomposition. For the sake of\ntime, we will not run the second ICA here, as there are no more pipeline calculations.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run ICLabel Classifier\n\nThe pipeline will run the ICLabel classifier on the final ICA, which will produce a\nlabel for each IC, one of ``\"brain\"``, ``\"muscle\"``, ``\"eog\"`` (eye), ``\"ecg\"``\n(heart), ``line_noise``, or ``\"channel_noise\"``.\n\n\n## Conclusion\nAnd that's all! See the other pylossless tutorials for brief examples on running the\npipeline on your own data, and rejecting the flagged data.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}